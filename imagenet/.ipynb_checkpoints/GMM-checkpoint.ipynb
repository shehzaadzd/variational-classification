{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/sdhuliawala/anaconda3/envs/softmax_variant/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", font=\"Arial\")\n",
    "colors = sns.color_palette(\"Paired\", n_colors=12).as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class full_model(nn.Module):\n",
    "    def __init__(self, args, network, num_classes=1000, num_feats=512 ) -> None:\n",
    "        super(full_model, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.network = network\n",
    "        if args.lgm:\n",
    "            self.criterion = LGM(args, num_classes, num_feats)\n",
    "\n",
    "        if args.vc:\n",
    "            self.criterion = VariationalClassification(args, num_classes, num_feats)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "        # tsampled_accuracy += ((Tsampled < 0) == torch.ones_like(Tsampled)).sum().item()\n",
    "        # treal_accuracy += ((Treal > 0) == torch.ones_like(Treal)).sum().item()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class Discriminators_2l(nn.Module):\n",
    "    def __init__(self, num_classes, feat_dim):\n",
    "        super(Discriminators_2l, self).__init__() \n",
    "        self.hidden_dim = feat_dim\n",
    "        self.W1 = nn.Parameter(torch.randn(num_classes, self.hidden_dim, feat_dim))\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_classes, self.hidden_dim))\n",
    "        self.W2 = nn.Parameter(torch.randn(num_classes, self.hidden_dim))\n",
    "        self.b2 = nn.Parameter(torch.zeros(num_classes))\n",
    "        self.relu  = nn.Tanh()\n",
    "        # self.classifiers = []\n",
    "        # for i in range(num_classes):\n",
    "        #     self.classifiers.append(nn.Sequential(nn.Linear(feat_dim, feat_dim), nn.ReLU(), nn.Linear(feat_dim, num_classes)))\n",
    "            \n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, Z, y):\n",
    "        #Z Bxfeat_dim\n",
    "        #y B\n",
    "\n",
    "        w1 = self.W1[y, :, :] #B x 2*feat_dim x feat_dim\n",
    "        w2 = self.W2[y, :] #B x 2* feat_dim x 1\n",
    "        b1 = self.b1[y, :] # B x 2* feat\n",
    "        b2 = self.b2[y, ] # B x 1\n",
    "\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        op = self.relu((w1 * Z.unsqueeze(1)).sum(-1) + b1)\n",
    "        op =(w2*op).sum(-1) + b2\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        # op = (w2* Z).sum(-1) + b2\n",
    "\n",
    "        return op\n",
    "\n",
    "    def reset(self,):\n",
    "        torch.nn.init.xavier_uniform_(self.W1)\n",
    "        torch.nn.init.zeros_(self.b1)\n",
    "        torch.nn.init.xavier_uniform_(self.W2)\n",
    "        torch.nn.init.zeros_(self.b2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pairwise_dist(mat, num_classes):\n",
    "    dist = 0.0\n",
    "\n",
    "    for i in range(num_classes-1):\n",
    "        for j in range(num_classes-1, i, -1):\n",
    "\n",
    "            dist += (mat[i, :]- mat[j, :]).pow(2).sum(-1).sqrt()\n",
    "    return dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Discriminators_1l(nn.Module):\n",
    "    def __init__(self, num_classes, feat_dim):\n",
    "        super(Discriminators_1l, self).__init__() \n",
    "        self.W1 = nn.Parameter(torch.randn(num_classes, feat_dim))\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_classes))\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, Z, y):\n",
    "        #Z Bxfeat_dim\n",
    "        #y B\n",
    "\n",
    "        w1 = self.W1[y, :] #B x feat_dim x feat_dim\n",
    "        b1 = self.b1[y] # B x feat\n",
    "\n",
    "\n",
    "\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        # Z_ = torch.concat([Z, Z*Z], dim=-1)\n",
    "\n",
    "        op = (w1 * Z).sum(-1) + b1\n",
    "        # op =(w2*op).sum(-1) + b1\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        # op = (w2* Z).sum(-1) + b2\n",
    "\n",
    "        return op\n",
    "    def reset(self,):\n",
    "        torch.nn.init.xavier_uniform_(self.W1)\n",
    "        torch.nn.init.zeros_(self.b1)\n",
    "\n",
    "class VariationalClassification(nn.Module):\n",
    "    def __init__(self, args, num_classes, feat_dim , ) -> None:\n",
    "        super(VariationalClassification, self).__init__()\n",
    "        self.NLL = nn.CrossEntropyLoss()\n",
    "        self.VC = AdversarialContrastiveLoss(num_classes, feat_dim, alpha=0.0)\n",
    "\n",
    "        self.classifier = Discriminators_2l(num_classes=num_classes, feat_dim=feat_dim) if args.disc_layers == 2 else Discriminators_1l(num_classes=num_classes, feat_dim=feat_dim)\n",
    "\n",
    "        self.disc_optimizer = torch.optim.Adam( [p for p in self.classifier.parameters()]   ,  lr=9e-4, weight_decay=1e-4)\n",
    "\n",
    "        self.args = args\n",
    "    def forward(self,outputs, targets ):\n",
    "        args = self.args\n",
    "        logits, self.real, self.sampled, likelihood = self.VC(outputs, label= targets, detach_features=True)\n",
    "        self.targets = targets\n",
    "\n",
    "        Treal = self.classifier(outputs, targets).squeeze(-1)\n",
    "        # from losses import pairwise_dist\n",
    "        # dist = pairwise_dist(self.criterion.centers, 10)\n",
    "\n",
    "\n",
    "        # loss_1 = self.cross_entropy(logits, targets)  + args.l1*(likelihood.mean())   #+ 0.001 * (0.2*torch.exp(self.criterion.log_covs).sum() - 640*self.criterion.log_covs.sum()) \n",
    "\n",
    "        loss_1 = self.NLL(logits, targets)  + args.l1*(likelihood.mean())  #+ 0.01 * (torch.exp(self.criterion.log_covs).mean() - dist) \n",
    "        loss_2 = Treal.mean()\n",
    "        loss = loss_1 + args.l2*loss_2\n",
    "        return loss, logits\n",
    "\n",
    "    def discriminator_train(self):\n",
    "        Tsampled = self.classifier(self.sampled.detach(), self.targets).squeeze(-1)\n",
    "        Treal = self.classifier(self.real.detach(), self.targets).squeeze(-1)\n",
    "        self.disc_optimizer.zero_grad()\n",
    "        dual_loss = ( F.binary_cross_entropy_with_logits(Treal, torch.ones_like(Treal)) + F.binary_cross_entropy_with_logits(Tsampled, torch.zeros_like(Tsampled)) )\n",
    "        dual_loss.backward()\n",
    "        # print(\"Treal acc: {}\".format(((Treal > 0) == torch.ones_like(Treal)).detach().cpu().numpy().mean()))\n",
    "        # print(\"Tsampled acc: {}\".format(((Tsampled < 0) == torch.zeros_like(Tsampled)).detach().cpu().numpy().mean()))\n",
    "        self.disc_optimizer.step()\n",
    "        self.disc_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "class LGM(nn.Module):\n",
    "    def __init__(self, args, num_classes, feat_dim , ) -> None:\n",
    "        super(LGM, self).__init__()\n",
    "        self.NLL = nn.CrossEntropyLoss()\n",
    "        self.VC = AdversarialContrastiveLoss(num_classes, feat_dim, alpha=0.0)\n",
    "\n",
    "        # self.classifier = Discriminators_2l(num_classes=num_classes, feat_dim=feat_dim) if args.disc_layers == 2 else Discriminators_1l(num_classes=num_classes, feat_dim=feat_dim)\n",
    "\n",
    "        # self.disc_optimizer = torch.optim.Adam( [p for p in self.classifier.parameters()]   ,  lr=1e-3, weight_decay=2e-3)\n",
    "\n",
    "        self.args = args\n",
    "    def forward(self,outputs, targets ):\n",
    "        args = self.args\n",
    "        logits, self.real, self.sampled, likelihood = self.VC(outputs, label= targets, detach_features=False)\n",
    "        self.targets = targets\n",
    "\n",
    "        # from losses import pairwise_dist\n",
    "        # dist = pairwise_dist(self.criterion.centers, 10)\n",
    "\n",
    "\n",
    "        # loss_1 = self.cross_entropy(logits, targets)  + args.l1*(likelihood.mean())   #+ 0.001 * (0.2*torch.exp(self.criterion.log_covs).sum() - 640*self.criterion.log_covs.sum()) \n",
    "        loss_1 = self.NLL(logits, targets)  + args.l1*(likelihood.mean())  #+ 0.01 * (torch.exp(self.criterion.log_covs).mean() - dist) \n",
    "        loss = loss_1 \n",
    "        return loss, logits\n",
    "\n",
    "\n",
    "\n",
    "class AdversarialContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Refer to paper:\n",
    "    Weitao Wan, Yuanyi Zhong,Tianpeng Li, Jiansheng Chen\n",
    "    Rethinking Feature Distribution for Loss Functions in Image Classification. CVPR 2018\n",
    "    re-implement by yirong mao\n",
    "    2018 07/02\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, feat_dim , alpha):\n",
    "        super(AdversarialContrastiveLoss, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "        #Theta\n",
    "        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))\n",
    "        # self.centers = torch.randn(num_classes, feat_dim).cuda()\n",
    "        # self.centers = nn.Parameter(torch.cat([torch.eye(num_classes), torch.zeros(num_classes, feat_dim-num_classes)], dim=-1))\n",
    "\n",
    "        # self.log_covs = nn.Parameter(torch.zeros(num_classes, feat_dim))\n",
    "        self.log_covs = torch.zeros(num_classes, feat_dim).cuda()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, feat, label=None, detach_features=False):\n",
    "\n",
    "        batch_size = feat.shape[0]\n",
    "        feat_dim = feat.shape[1]\n",
    "        log_covs = torch.unsqueeze(self.log_covs, dim=0)\n",
    "\n",
    "        covs = torch.exp(log_covs)  # 1*c*d\n",
    "        tcovs = covs.repeat(batch_size, 1, 1)  # n*c*d\n",
    "        # try:\n",
    "        diff = torch.unsqueeze(feat, dim=1) - torch.unsqueeze(self.centers, dim=0)\n",
    "        # except RuntimeError as e:\n",
    "        #     print(feat.shape)\n",
    "        #     print(self.centers.shape)\n",
    "        #     import pdb\n",
    "        #     pdb.set_trace()\n",
    "        wdiff = torch.div(diff, tcovs)\n",
    "        diff = torch.mul(diff, wdiff)\n",
    "        dist = torch.sum(diff, dim=-1)  # eq.(18)\n",
    "        # if label == None:\n",
    "        #     label = torch.argmin(dist, dim=-1)\n",
    "\n",
    "\n",
    "        slog_covs = torch.sum(log_covs, dim=-1)  # 1*c\n",
    "        tslog_covs = slog_covs.repeat(batch_size, 1)\n",
    "        logits = -0.5 * (tslog_covs + dist)\n",
    "        likelihood_logits = logits\n",
    "\n",
    "        if label == None:\n",
    "            label = torch.argmin(dist, dim=-1)\n",
    "\n",
    "        if detach_features:            \n",
    "            diff = torch.unsqueeze(feat.detach(), dim=1) - \\\n",
    "                torch.unsqueeze(self.centers, dim=0)\n",
    "            wdiff = torch.div(diff, tcovs)\n",
    "            diff = torch.mul(diff, wdiff)\n",
    "            dist = torch.sum(diff, dim=-1)  # eq.(18)\n",
    "            # if label == None:\n",
    "            #     label = torch.argmin(dist, dim=-1)\n",
    "\n",
    "\n",
    "            slog_covs = torch.sum(log_covs, dim=-1)  # 1*c\n",
    "            tslog_covs = slog_covs.repeat(batch_size, 1)\n",
    "            logits_detached = -0.5 * (tslog_covs + dist)\n",
    "            likelihood_logits = logits_detached\n",
    "\n",
    "\n",
    "\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "\n",
    "\n",
    "        Treal = None\n",
    "        Tsampled = None\n",
    "        # if label != None:\n",
    "        #     Treal = feat \n",
    "\n",
    "        #     # try:\n",
    "        #     distrib = Normal(loc=self.centers[label].reshape(-1), scale = torch.exp(self.log_covs[label]).reshape(-1) )\n",
    "        #     # except ValueError as E:\n",
    "        #     #     import pdb\n",
    "        #     #     pdb.set_trace()\n",
    "        #     Tsampled = distrib.sample().cuda().reshape(-1, feat_dim)\n",
    "        #     # Tsampled = feat\n",
    "        # likelihood = -likelihood_logits[torch.arange(batch_size), label]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if label != None:\n",
    "            Treal = feat \n",
    "\n",
    "            # try:\n",
    "            # distrib = MultivariateNormal(loc=self.centers[label], covariance_matrix=torch.eye(feat_dim).repeat(batch_size,1,1).cuda() *  torch.exp(self.log_covs[label]).unsqueeze(-1))\n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            # except ValueError as E:\n",
    "            #     import pdb\n",
    "            #     pdb.set_trace()\n",
    "\n",
    "            Tsampled = (torch.Tensor(batch_size, feat_dim).normal_().cuda() * torch.sqrt(torch.exp(self.log_covs[label])) + self.centers[label].cuda())\n",
    "            # Tsampled2 = (distrib.sample().cuda() * torch.sqrt(torch.exp(self.log_covs[label])) + self.centers[label].cuda())\n",
    "            # Tsampled = torch.cat([Tsampled1, Tsampled2], dim=0).detach()\n",
    " \n",
    "            # import pdb\n",
    "            # pdb.set_trace()\n",
    "        likelihood = -likelihood_logits[torch.arange(batch_size), label]\n",
    "\n",
    "        return logits, Treal.detach(), Tsampled.detach(), likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(l1 = 0.1, l2 = 0.1, vc=False, lgm=False, ce=True, disc_layers = 1, dim=2, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1000\n",
    "n_test = 100\n",
    "d = args.dim\n",
    "classes = args.num_classes\n",
    "\n",
    "means = torch.Tensor(classes, d).normal_()*10\n",
    "    # .. and shift them around to non-standard Gaussians\n",
    "covs = torch.ones(classes, d) \n",
    "\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "\n",
    "        \n",
    "for i in range(n_train):\n",
    "    for label in range(classes):\n",
    "        \n",
    "        distrib = torch.Tensor(d).normal_()\n",
    "    \n",
    "        s = distrib * torch.sqrt(covs[label, :]) + means[label, :]\n",
    "        train_y.append(label)\n",
    "        train_x.append(s)\n",
    "means += torch.Tensor(classes, d).normal_() \n",
    "for i in range(n_test):\n",
    "    for label in range(classes):\n",
    "        \n",
    "        distrib = torch.Tensor(d).normal_()\n",
    "    \n",
    "        s = distrib * torch.sqrt(covs[label, :]) + means[label, :]\n",
    "        test_y.append(label)\n",
    "        test_x.append(s)\n",
    "\n",
    "train_x = torch.stack(train_x, dim=0)\n",
    "train_y = torch.LongTensor(train_y)\n",
    "\n",
    "test_x = torch.stack(test_x, dim=0)\n",
    "test_y = torch.LongTensor(test_y)\n",
    "\n",
    "\n",
    "train_ds = TensorDataset(train_x, train_y)\n",
    "test_ds = TensorDataset(test_x, test_y)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=1024, shuffle=True, num_workers=8)\n",
    "test_dl = DataLoader(test_ds, batch_size=2048, num_workers = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.figure().clear()\n",
    "plt.close()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.scatter(x=train_x[:,0], y=train_x[:, 1])\n",
    "plt.scatter(x=test_x[:,0], y=test_x[:, 1])\n",
    "plt.savefig(\"data.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "    def __init__(self, dim = 10, num_classes = 100):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(dim, dim), torch.nn.Tanh(), nn.Linear(dim, dim))\n",
    "        self.lin = nn.Linear(dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "class network1(nn.Module):\n",
    "    def __init__(self, dim = 10, num_classes = 100):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(dim, dim)\n",
    "        self.lin = nn.Linear(dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "if args.vc or args.lgm:\n",
    "    net = network(dim=args.dim, num_classes = args.num_classes).cuda()\n",
    "    model = full_model(network=net, args=args, num_classes=args.num_classes, num_feats=args.dim).cuda()\n",
    "else:\n",
    "    model = network(dim=args.dim, num_classes = args.num_classes).cuda()\n",
    "\n",
    "if args.vc or args.lgm:\n",
    "    optimizer_2 = torch.optim.SGD([p for p in model.criterion.VC.parameters()],lr=0.01 )\n",
    "    optimizer_1 = torch.optim.SGD([p for p in model.network.parameters()], lr=0.01)\n",
    "\n",
    "    criterion = model.criterion\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer_2 = None\n",
    "    optimizer_1 = torch.optim.Adam([p for p in model.parameters()])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6190098524093628\n",
      "Accuracy: 0.197\n",
      "ECE: 17.6784133797884\n",
      "1.6185319493001387\n",
      "Accuracy: 0.1974\n",
      "ECE: 17.648651630580424\n",
      "1.6180991704585437\n",
      "Accuracy: 0.1978\n",
      "ECE: 17.635757595598697\n",
      "1.6174373027503184\n",
      "Accuracy: 0.198\n",
      "ECE: 17.58661621540785\n",
      "1.6166732802086468\n",
      "Accuracy: 0.198\n",
      "ECE: 17.526279639303684\n",
      "1.6156846222330836\n",
      "Accuracy: 0.1982\n",
      "ECE: 17.482070382535458\n",
      "1.6147540597563284\n",
      "Accuracy: 0.1982\n",
      "ECE: 17.40840059131384\n",
      "1.6136342882083965\n",
      "Accuracy: 0.1984\n",
      "ECE: 17.358012440204618\n",
      "1.612353584463792\n",
      "Accuracy: 0.1986\n",
      "ECE: 17.3059077963233\n",
      "1.6110255340281825\n",
      "Accuracy: 0.199\n",
      "ECE: 17.267948443591592\n",
      "1.6095873513470003\n",
      "Accuracy: 0.199\n",
      "ECE: 17.188887337744234\n",
      "1.607976194656722\n",
      "Accuracy: 0.199\n",
      "ECE: 17.09521143645048\n",
      "1.6062903605234082\n",
      "Accuracy: 0.199\n",
      "ECE: 17.00354207187891\n",
      "1.6043402350628486\n",
      "Accuracy: 0.199\n",
      "ECE: 16.90524053543806\n",
      "1.6024046887099876\n",
      "Accuracy: 0.199\n",
      "ECE: 16.80406642705202\n",
      "1.6003155324349283\n",
      "Accuracy: 0.1992\n",
      "ECE: 16.71880835771561\n",
      "1.597965437197662\n",
      "Accuracy: 0.1992\n",
      "ECE: 16.604429086148738\n",
      "1.59547667442921\n",
      "Accuracy: 0.1992\n",
      "ECE: 16.47661573112011\n",
      "1.5929038552054726\n",
      "Accuracy: 0.1992\n",
      "ECE: 16.347491558492184\n",
      "1.5902467770401387\n",
      "Accuracy: 0.1994\n",
      "ECE: 16.228064628243445\n",
      "1.5873320117559033\n",
      "Accuracy: 0.1994\n",
      "ECE: 16.086641345322132\n",
      "1.5843343894829025\n",
      "Accuracy: 0.1994\n",
      "ECE: 17.51174848735332\n",
      "1.5810292534105932\n",
      "Accuracy: 0.1996\n",
      "ECE: 15.170947638154031\n",
      "1.5776546456525204\n",
      "Accuracy: 0.1996\n",
      "ECE: 10.350143999755387\n",
      "1.5739553167327751\n",
      "Accuracy: 0.1996\n",
      "ECE: 8.40392947643995\n",
      "1.5700664812084932\n",
      "Accuracy: 0.3996\n",
      "ECE: 14.053313449919225\n",
      "1.566113934143017\n",
      "Accuracy: 0.3996\n",
      "ECE: 13.893791506886489\n",
      "1.5617920955095832\n",
      "Accuracy: 0.3996\n",
      "ECE: 13.726635094583036\n",
      "1.5573319707144733\n",
      "Accuracy: 0.3996\n",
      "ECE: 13.555629189908508\n",
      "1.552686690255127\n",
      "Accuracy: 0.3996\n",
      "ECE: 13.379561334252358\n",
      "1.547924304252141\n",
      "Accuracy: 0.3996\n",
      "ECE: 13.198921418190007\n",
      "1.542846918690518\n",
      "Accuracy: 0.3996\n",
      "ECE: 13.014712020158766\n",
      "1.5375607528114876\n",
      "Accuracy: 0.3998\n",
      "ECE: 12.838261021077631\n",
      "1.5321650757429806\n",
      "Accuracy: 0.3998\n",
      "ECE: 12.638808749914174\n",
      "1.5264678547924582\n",
      "Accuracy: 0.3998\n",
      "ECE: 12.437373381257059\n",
      "1.5205748387096047\n",
      "Accuracy: 0.3998\n",
      "ECE: 12.229020165205002\n",
      "1.5144529633009027\n",
      "Accuracy: 0.3998\n",
      "ECE: 12.01047274559736\n",
      "1.5082285903359263\n",
      "Accuracy: 0.3998\n",
      "ECE: 11.790492400825023\n",
      "1.5019184734501467\n",
      "Accuracy: 0.3998\n",
      "ECE: 11.578685033321378\n",
      "1.4951603223586185\n",
      "Accuracy: 0.3998\n",
      "ECE: 11.359065352380275\n",
      "1.4885000338802157\n",
      "Accuracy: 0.4\n",
      "ECE: 11.156059571206569\n",
      "1.4815952149336815\n",
      "Accuracy: 0.4\n",
      "ECE: 10.936460091173648\n",
      "1.4744553953936037\n",
      "Accuracy: 0.4\n",
      "ECE: 10.71443015575409\n",
      "1.4673564430128447\n",
      "Accuracy: 0.4\n",
      "ECE: 10.491753914058208\n",
      "1.4601300509014659\n",
      "Accuracy: 0.4\n",
      "ECE: 10.264300962090491\n",
      "1.4525930844150883\n",
      "Accuracy: 0.4\n",
      "ECE: 10.037653724849223\n",
      "1.445158378043522\n",
      "Accuracy: 0.4\n",
      "ECE: 9.81207243829966\n",
      "1.4374332653350843\n",
      "Accuracy: 0.4\n",
      "ECE: 9.58947329133749\n",
      "1.4297811950017785\n",
      "Accuracy: 0.3944\n",
      "ECE: 8.791599059402945\n",
      "1.422052632664209\n",
      "Accuracy: 0.4\n",
      "ECE: 9.072261013686662\n"
     ]
    }
   ],
   "source": [
    "curr_loss = None\n",
    "args.vis = True\n",
    "for epoch_no in range(50):\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    confidences = []\n",
    "    feats = []\n",
    "    for id_, (x, y) in enumerate(train_dl):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        z = model(x)\n",
    "        if args.vis:\n",
    "            feats.append(z.detach().cpu().numpy())\n",
    "\n",
    "        if args.ce:\n",
    "            logits = model.lin(z)\n",
    "        \n",
    "        \n",
    "\n",
    "        if args.vc or args.lgm:\n",
    "            loss, logits = criterion(z, y)\n",
    "        else:\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "        _, preds = logits.max(-1)\n",
    "        \n",
    "        confs, _ = torch.max(F.softmax(logits, dim=-1), -1)\n",
    "        confidences.extend(confs.detach().cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "        # Backward pass and update\n",
    "\n",
    "        optimizer_1.zero_grad()\n",
    "        if args.vc or args.lgm:\n",
    "            optimizer_2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_1.step()\n",
    "        if args.vc or args.lgm:\n",
    "            optimizer_2.step()\n",
    "\n",
    "        if args.vc or args.lgm:\n",
    "            optimizer_1.zero_grad()\n",
    "            optimizer_2.zero_grad()\n",
    "            if args.vc:\n",
    "                criterion.discriminator_train()\n",
    "            optimizer_1.zero_grad()\n",
    "            optimizer_2.zero_grad()\n",
    "\n",
    "        \n",
    "#         loss = criterion(logits, y).mean()\n",
    "        \n",
    "        all_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "        all_targets.extend(y.cpu().numpy().tolist())\n",
    "        \n",
    "\n",
    "        loss_ = loss.item()\n",
    "        if curr_loss == None:\n",
    "            curr_loss = loss_\n",
    "        else:\n",
    "            curr_loss = curr_loss * 0.99 + loss_ * 0.01\n",
    "        if id_ % 100 == 0:\n",
    "            print(curr_loss)\n",
    "            \n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    accuracies = (all_preds == all_targets ).tolist()\n",
    "\n",
    "    ECE, MCE, _, _, _ =  Evaluator.create_fine_bins(accuracies, confidences)\n",
    "    ECE = ECE * 100\n",
    "    \n",
    "\n",
    "    acc = (all_preds == all_targets ).mean()\n",
    "    print(\"Accuracy: {}\".format(acc))\n",
    "    print(\"ECE: {}\".format(ECE))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.concatenate(feats, axis=0)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.figure().clear()\n",
    "plt.close()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.scatter(x=feats[:,0], y=feats[:, 1])\n",
    "plt.scatter(x=feats[:,0], y=feats[:, 1])\n",
    "plt.savefig(\"z.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54684\n",
      "ECE: 8.558109813943506\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_targets = []\n",
    "confidences = []\n",
    "for id_, (x, y) in enumerate(test_dl):\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    z = model(x)\n",
    "\n",
    "\n",
    "    if args.ce:\n",
    "        logits = model.lin(z)\n",
    "\n",
    "\n",
    "\n",
    "    if args.vc or args.lgm:\n",
    "        loss, logits = criterion(z, y)\n",
    "    else:\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "    _, preds = logits.max(-1)\n",
    "\n",
    "    \n",
    "    \n",
    "    confs, _ = torch.max(F.softmax(logits, dim=-1), -1)\n",
    "    confidences.extend(confs.detach().cpu().numpy().tolist())\n",
    "    \n",
    "    all_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "    all_targets.extend(y.cpu().numpy().tolist())\n",
    "\n",
    "    loss_ = loss.item()\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "acc = (all_preds == all_targets ).mean()\n",
    "accuracies = (all_preds == all_targets ).tolist()\n",
    "\n",
    "ECE, MCE, _, _, _ =  Evaluator.create_fine_bins(accuracies, confidences)\n",
    "ECE = ECE * 100\n",
    "\n",
    "\n",
    "acc = (all_preds == all_targets ).mean()\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "print(\"ECE: {}\".format(ECE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(0)\n",
    "plt.figure().clear()\n",
    "plt.close()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.scatter(x=torch.stack(samples, dim=0)[:, 0],y = torch.stack(samples, dim=0)[:, 1] )\n",
    "plt.savefig(\"data.png\")\n",
    "\n",
    "# plot(torch.stack(samples, dim=0), torch.LongTensor(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:softmax_variant]",
   "language": "python",
   "name": "conda-env-softmax_variant-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
